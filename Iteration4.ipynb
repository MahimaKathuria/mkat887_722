{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init('/home/ubuntu/spark-2.1.1-bin-hadoop2.7')\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('operations').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- age: double (nullable = true)\n",
      " |-- hypertension: integer (nullable = true)\n",
      " |-- heart_disease: integer (nullable = true)\n",
      " |-- ever_married: string (nullable = true)\n",
      " |-- work_type: string (nullable = true)\n",
      " |-- Residence_type: string (nullable = true)\n",
      " |-- avg_glucose_level: double (nullable = true)\n",
      " |-- bmi: double (nullable = true)\n",
      " |-- smoking_status: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- age: double (nullable = true)\n",
      " |-- hypertension: integer (nullable = true)\n",
      " |-- heart_disease: integer (nullable = true)\n",
      " |-- ever_married: string (nullable = true)\n",
      " |-- work_type: string (nullable = true)\n",
      " |-- Residence_type: string (nullable = true)\n",
      " |-- avg_glucose_level: double (nullable = true)\n",
      " |-- bmi: double (nullable = true)\n",
      " |-- smoking_status: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Loading datasets\n",
    "dataset1 = spark.read.csv('Datasets/traindataset.csv', inferSchema=True, header=True)\n",
    "dataset2 = spark.read.csv('Datasets/testdataset.csv', inferSchema=True, header=True)\n",
    "#Printing the types of attributes\n",
    "dataset1.printSchema()\n",
    "dataset2.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+----+------------+-------------+------------+-------------+--------------+-----------------+----+---------------+\n",
      "|   id|gender| age|hypertension|heart_disease|ever_married|    work_type|Residence_type|avg_glucose_level| bmi| smoking_status|\n",
      "+-----+------+----+------------+-------------+------------+-------------+--------------+-----------------+----+---------------+\n",
      "|30669|  Male| 3.0|           0|            0|          No|     children|         Rural|            95.12|18.0|           null|\n",
      "|30468|  Male|58.0|           1|            0|         Yes|      Private|         Urban|            87.96|39.2|   never smoked|\n",
      "|16523|Female| 8.0|           0|            0|          No|      Private|         Urban|           110.89|17.6|           null|\n",
      "|56543|Female|70.0|           0|            0|         Yes|      Private|         Rural|            69.04|35.9|formerly smoked|\n",
      "|46136|  Male|14.0|           0|            0|          No| Never_worked|         Rural|           161.28|19.1|           null|\n",
      "|32257|Female|47.0|           0|            0|         Yes|      Private|         Urban|           210.95|50.1|           null|\n",
      "|52800|Female|52.0|           0|            0|         Yes|      Private|         Urban|            77.59|17.7|formerly smoked|\n",
      "|41413|Female|75.0|           0|            1|         Yes|Self-employed|         Rural|           243.53|27.0|   never smoked|\n",
      "|15266|Female|32.0|           0|            0|         Yes|      Private|         Rural|            77.67|32.3|         smokes|\n",
      "|28674|Female|74.0|           1|            0|         Yes|Self-employed|         Urban|           205.84|54.6|   never smoked|\n",
      "|10460|Female|79.0|           0|            0|         Yes|     Govt_job|         Urban|            77.08|35.0|           null|\n",
      "|64908|  Male|79.0|           0|            1|         Yes|      Private|         Urban|            57.08|22.0|formerly smoked|\n",
      "|63884|Female|37.0|           0|            0|         Yes|      Private|         Rural|           162.96|39.4|   never smoked|\n",
      "|37893|Female|37.0|           0|            0|         Yes|      Private|         Rural|             73.5|26.1|formerly smoked|\n",
      "|67855|Female|40.0|           0|            0|         Yes|      Private|         Rural|            95.04|42.4|   never smoked|\n",
      "|25774|  Male|35.0|           0|            0|          No|      Private|         Rural|            85.37|33.0|   never smoked|\n",
      "|19584|Female|20.0|           0|            0|          No|      Private|         Urban|            84.62|19.7|         smokes|\n",
      "|24447|Female|42.0|           0|            0|         Yes|      Private|         Rural|            82.67|22.5|   never smoked|\n",
      "|49589|Female|44.0|           0|            0|         Yes|     Govt_job|         Urban|            57.33|24.6|         smokes|\n",
      "|17986|Female|79.0|           0|            1|         Yes|Self-employed|         Urban|            67.84|25.2|         smokes|\n",
      "+-----+------+----+------------+-------------+------------+-------------+--------------+-----------------+----+---------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+-----+------+----+------------+-------------+------------+-------------+--------------+-----------------+----+---------------+\n",
      "|   id|gender| age|hypertension|heart_disease|ever_married|    work_type|Residence_type|avg_glucose_level| bmi| smoking_status|\n",
      "+-----+------+----+------------+-------------+------------+-------------+--------------+-----------------+----+---------------+\n",
      "|36306|  Male|80.0|           0|            0|         Yes|      Private|         Urban|            83.84|21.1|formerly smoked|\n",
      "|61829|Female|74.0|           0|            1|         Yes|Self-employed|         Rural|            179.5|26.0|formerly smoked|\n",
      "|14152|Female|14.0|           0|            0|          No|     children|         Rural|            95.16|21.2|           null|\n",
      "|12997|  Male|28.0|           0|            0|          No|      Private|         Urban|            94.76|23.4|           null|\n",
      "|40801|Female|63.0|           0|            0|         Yes|     Govt_job|         Rural|            83.57|27.6|   never smoked|\n",
      "| 9348|Female|66.0|           1|            0|         Yes|      Private|         Urban|           219.98|32.2|   never smoked|\n",
      "|51550|Female|49.0|           0|            0|         Yes|Self-employed|         Rural|            74.03|25.1|           null|\n",
      "|60512|  Male|46.0|           0|            0|         Yes|     Govt_job|         Urban|            120.8|32.5|   never smoked|\n",
      "|31309|Female|75.0|           0|            0|         Yes|Self-employed|         Rural|            78.71|28.0|   never smoked|\n",
      "|39199|  Male|75.0|           0|            0|         Yes|Self-employed|         Urban|             77.2|25.7|         smokes|\n",
      "|15160|Female|17.0|           0|            0|          No|      Private|         Rural|            78.16|21.9|           null|\n",
      "|21705|Female|10.0|           0|            0|          No|     children|         Urban|           107.23|19.4|           null|\n",
      "|19042|Female|47.0|           0|            0|         Yes|      Private|         Rural|             91.6|26.7|   never smoked|\n",
      "|12249|Female|42.0|           0|            0|         Yes|      Private|         Urban|            83.05|32.3|           null|\n",
      "|33104|Female|67.0|           0|            0|         Yes|     Govt_job|         Urban|            236.6|24.2|   never smoked|\n",
      "|55264|Female|52.0|           0|            0|          No|Self-employed|         Urban|           109.49|24.5|   never smoked|\n",
      "|29445|  Male|73.0|           0|            0|         Yes|Self-employed|         Rural|           109.66|40.0|           null|\n",
      "|49013|Female|19.0|           0|            0|          No|      Private|         Rural|            88.51|22.1|           null|\n",
      "|  276|  Male|15.0|           0|            0|          No|     children|         Rural|           101.36|22.3|           null|\n",
      "|47721|Female|37.0|           0|            0|         Yes|     Govt_job|         Urban|           165.44|36.1|formerly smoked|\n",
      "+-----+------+----+------------+-------------+------------+-------------+--------------+-----------------+----+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Displaying datasets\n",
    "dataset1.show()\n",
    "dataset2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43400"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Counting number of rows of dataset1\n",
    "dataset1.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Counting number of columns of dataset1\n",
    "len(dataset1.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18601"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Counting number of rows of dataset2\n",
    "dataset2.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Counting number of columns of dataset2\n",
    "len(dataset2.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+----+------------+-------------+------------+-------------+--------------+-----------------+----+---------------+\n",
      "|   id|gender| age|hypertension|heart_disease|ever_married|    work_type|Residence_type|avg_glucose_level| bmi| smoking_status|\n",
      "+-----+------+----+------------+-------------+------------+-------------+--------------+-----------------+----+---------------+\n",
      "|30669|  Male| 3.0|           0|            0|          No|     children|         Rural|            95.12|18.0|           null|\n",
      "|30468|  Male|58.0|           1|            0|         Yes|      Private|         Urban|            87.96|39.2|   never smoked|\n",
      "|16523|Female| 8.0|           0|            0|          No|      Private|         Urban|           110.89|17.6|           null|\n",
      "|56543|Female|70.0|           0|            0|         Yes|      Private|         Rural|            69.04|35.9|formerly smoked|\n",
      "|46136|  Male|14.0|           0|            0|          No| Never_worked|         Rural|           161.28|19.1|           null|\n",
      "|32257|Female|47.0|           0|            0|         Yes|      Private|         Urban|           210.95|50.1|           null|\n",
      "|52800|Female|52.0|           0|            0|         Yes|      Private|         Urban|            77.59|17.7|formerly smoked|\n",
      "|41413|Female|75.0|           0|            1|         Yes|Self-employed|         Rural|           243.53|27.0|   never smoked|\n",
      "|15266|Female|32.0|           0|            0|         Yes|      Private|         Rural|            77.67|32.3|         smokes|\n",
      "|28674|Female|74.0|           1|            0|         Yes|Self-employed|         Urban|           205.84|54.6|   never smoked|\n",
      "|10460|Female|79.0|           0|            0|         Yes|     Govt_job|         Urban|            77.08|35.0|           null|\n",
      "|64908|  Male|79.0|           0|            1|         Yes|      Private|         Urban|            57.08|22.0|formerly smoked|\n",
      "|63884|Female|37.0|           0|            0|         Yes|      Private|         Rural|           162.96|39.4|   never smoked|\n",
      "|37893|Female|37.0|           0|            0|         Yes|      Private|         Rural|             73.5|26.1|formerly smoked|\n",
      "|67855|Female|40.0|           0|            0|         Yes|      Private|         Rural|            95.04|42.4|   never smoked|\n",
      "|25774|  Male|35.0|           0|            0|          No|      Private|         Rural|            85.37|33.0|   never smoked|\n",
      "|19584|Female|20.0|           0|            0|          No|      Private|         Urban|            84.62|19.7|         smokes|\n",
      "|24447|Female|42.0|           0|            0|         Yes|      Private|         Rural|            82.67|22.5|   never smoked|\n",
      "|49589|Female|44.0|           0|            0|         Yes|     Govt_job|         Urban|            57.33|24.6|         smokes|\n",
      "|17986|Female|79.0|           0|            1|         Yes|Self-employed|         Urban|            67.84|25.2|         smokes|\n",
      "+-----+------+----+------------+-------------+------------+-------------+--------------+-----------------+----+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Appending two datasets\n",
    "concat = dataset1.union(dataset2)\n",
    "concat.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "62001"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Counting number of rows of the merged dataset\n",
    "concat.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Counting number of columns of the merged dataset\n",
    "len(concat.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+\n",
      "|gender|count|\n",
      "+------+-----+\n",
      "|Female|36622|\n",
      "| Other|   13|\n",
      "|  Male|25366|\n",
      "+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Counting number of records in each type of attribute gender\n",
    "concat.groupby('gender').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----+\n",
      "|hypertension|count|\n",
      "+------------+-----+\n",
      "|           1| 5794|\n",
      "|           0|56207|\n",
      "+------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Counting number of records in each type of attribute hypertension\n",
    "concat.groupby('hypertension').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----+\n",
      "|    work_type|count|\n",
      "+-------------+-----+\n",
      "| Never_worked|  252|\n",
      "|Self-employed| 9654|\n",
      "|      Private|35584|\n",
      "|     children| 8769|\n",
      "|     Govt_job| 7742|\n",
      "+-------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Counting number of records in each type of attribute work_type\n",
    "concat.groupby('work_type').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----+\n",
      "|residence_type|count|\n",
      "+--------------+-----+\n",
      "|         Urban|31066|\n",
      "|         Rural|30935|\n",
      "+--------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Counting number of records in each type of attribute residence_type\n",
    "concat.groupby('residence_type').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----+\n",
      "|ever_married|count|\n",
      "+------------+-----+\n",
      "|          No|22124|\n",
      "|         Yes|39877|\n",
      "+------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Counting number of records in each type of attribute ever_married\n",
    "concat.groupby('ever_married').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----+\n",
      "| smoking_status|count|\n",
      "+---------------+-----+\n",
      "|           null|19043|\n",
      "|         smokes| 9319|\n",
      "|   never smoked|22886|\n",
      "|formerly smoked|10753|\n",
      "+---------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Counting number of records in each type of attribute smoking_status\n",
    "concat.groupby('smoking_status').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----+\n",
      "|heart_disease|count|\n",
      "+-------------+-----+\n",
      "|            1| 2956|\n",
      "|            0|59045|\n",
      "+-------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Counting number of records in each type of attribute heart_disease\n",
    "concat.groupby('heart_disease').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+------+------------------+-------------------+-------------------+------------+---------+--------------+-----------------+-----------------+---------------+\n",
      "|summary|               id|gender|               age|       hypertension|      heart_disease|ever_married|work_type|Residence_type|avg_glucose_level|              bmi| smoking_status|\n",
      "+-------+-----------------+------+------------------+-------------------+-------------------+------------+---------+--------------+-----------------+-----------------+---------------+\n",
      "|  count|            62001| 62001|             62001|              62001|              62001|       62001|    62001|         62001|            62001|            59948|          42958|\n",
      "|   mean|36452.51481427719|  null|42.169475331042975|0.09345010564345736|0.04767665037660683|        null|     null|          null| 104.453831712391| 28.5870988189766|           null|\n",
      "| stddev|21067.15594488889|  null|22.522099535371346| 0.2910645114143387|0.21308289396711122|        null|     null|          null| 42.9605371949353|7.766122190731495|           null|\n",
      "|    min|                1|Female|              0.08|                  0|                  0|          No| Govt_job|         Rural|             55.0|             10.1|formerly smoked|\n",
      "|    max|            72943| Other|              82.0|                  1|                  1|         Yes| children|         Urban|           291.05|             97.6|         smokes|\n",
      "+-------+-----------------+------+------------------+-------------------+-------------------+------------+---------+--------------+-----------------+-----------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Statistical summary of data\n",
    "concat.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+----+\n",
      "|gender_heart_disease|    0|   1|\n",
      "+--------------------+-----+----+\n",
      "|               Other|   13|   0|\n",
      "|                Male|23606|1760|\n",
      "|              Female|35426|1196|\n",
      "+--------------------+-----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Comparison between different types of gender having heart attacks\n",
    "concat.crosstab('gender', 'heart_disease').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------+-----+---+\n",
      "|smoking_status_heart_disease|    0|  1|\n",
      "+----------------------------+-----+---+\n",
      "|             formerly smoked| 9818|935|\n",
      "|                      smokes| 8713|606|\n",
      "|                never smoked|22024|862|\n",
      "|                        null|18490|553|\n",
      "+----------------------------+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Comparison between different types of smoking status having heart attacks\n",
    "concat.crosstab('smoking_status', 'heart_disease').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------+-----+----+\n",
      "|ever_married_heart_disease|    0|   1|\n",
      "+--------------------------+-----+----+\n",
      "|                        No|21893| 231|\n",
      "|                       Yes|37152|2725|\n",
      "+--------------------------+-----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Comparison between different types of marital status having heart attacks\n",
    "concat.crosstab('ever_married', 'heart_disease').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+-----+----+\n",
      "|work_type_heart_disease|    0|   1|\n",
      "+-----------------------+-----+----+\n",
      "|               children| 8765|   4|\n",
      "|           Never_worked|  251|   1|\n",
      "|          Self-employed| 8718| 936|\n",
      "|                Private|33946|1638|\n",
      "|               Govt_job| 7365| 377|\n",
      "+-----------------------+-----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Comparison between different types of work type having heart attacks\n",
    "concat.crosstab('work_type', 'heart_disease').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------+-----+----+\n",
      "|residence_type_heart_disease|    0|   1|\n",
      "+----------------------------+-----+----+\n",
      "|                       Rural|29441|1494|\n",
      "|                       Urban|29604|1462|\n",
      "+----------------------------+-----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "concat.crosstab('residence_type', 'heart_disease').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------+-----+----+\n",
      "|hypertension_heart_disease|    0|   1|\n",
      "+--------------------------+-----+----+\n",
      "|                         1| 5040| 754|\n",
      "|                         0|54005|2202|\n",
      "+--------------------------+-----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Comparison between patients with or without hypertension having heart attacks\n",
    "concat.crosstab('hypertension', 'heart_disease').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['gender',\n",
       " 'age',\n",
       " 'hypertension',\n",
       " 'heart_disease',\n",
       " 'ever_married',\n",
       " 'work_type',\n",
       " 'Residence_type',\n",
       " 'avg_glucose_level',\n",
       " 'bmi',\n",
       " 'smoking_status']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Dropping column 'id'\n",
    "concat.drop('id').columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col,sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+---+------------+-------------+------------+---------+--------------+-----------------+----+--------------+\n",
      "| id|gender|age|hypertension|heart_disease|ever_married|work_type|Residence_type|avg_glucose_level| bmi|smoking_status|\n",
      "+---+------+---+------------+-------------+------------+---------+--------------+-----------------+----+--------------+\n",
      "|  0|     0|  0|           0|            0|           0|        0|             0|                0|2053|         19043|\n",
      "+---+------+---+------------+-------------+------------+---------+--------------+-----------------+----+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Finding missing values\n",
    "concat.select(*(sum(col(c).isNull().cast(\"int\")).alias(c) for c in concat.columns)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Identifying outliers in attribute bmi\n",
    "df1 = concat.filter(concat['bmi'] < 10).select('bmi')\n",
    "df1.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "838"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Identifying outliers in attribute bmi\n",
    "df2 = concat.filter(concat['bmi'] > 50).select('bmi')\n",
    "df2.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Identifying outliers in attribute age\n",
    "df3 = concat.filter(concat['age'] < 0).select('age')\n",
    "df3.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Identifying outliers in attribute age\n",
    "df4 = concat.filter(concat['age'] > 100).select('age')\n",
    "df4.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2443"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Identifying outliers in attribute average glucose level\n",
    "df5 = concat.filter(concat['avg_glucose_level'] < 60).select('avg_glucose_level')\n",
    "df5.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "75"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Identifying outliers in attribute average glucose level\n",
    "df6 = concat.filter(concat['avg_glucose_level'] > 260).select('avg_glucose_level')\n",
    "df6.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41495"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Removing missing values\n",
    "clean2 = concat.dropna()\n",
    "clean2.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41495"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Removing outliers in attribute bmi\n",
    "c1 = clean2.filter(concat['bmi'] > 10).select('id','gender','age','hypertension','heart_disease','work_type','Residence_type','ever_married','bmi','avg_glucose_level','smoking_status')\n",
    "c1.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40828"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Removing outliers in attribute bmi\n",
    "c2 = c1.filter(concat['bmi'] < 50).select('id','gender','age','hypertension','heart_disease','work_type','Residence_type','ever_married','bmi','avg_glucose_level','smoking_status')\n",
    "c2.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40828"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Removing outliers in attribute age\n",
    "c3 = c2.filter(concat['age'] > 0).select('id','gender','age','hypertension','heart_disease','work_type','Residence_type','ever_married','bmi','avg_glucose_level','smoking_status')\n",
    "c3.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40828"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Removing outliers in attribute age\n",
    "c4 = c3.filter(concat['age'] <100).select('id','gender','age','hypertension','heart_disease','work_type','Residence_type','ever_married','bmi','avg_glucose_level','smoking_status')\n",
    "c4.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39197"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Removing outliers in attribute average glucose level\n",
    "c5 = c4.filter(concat['avg_glucose_level'] > 60).select('id','gender','age','hypertension','heart_disease','work_type','Residence_type','ever_married','bmi','avg_glucose_level','smoking_status')\n",
    "c5.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39137"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Removing outliers in attribute average glucose level\n",
    "c6 = c5.filter(concat['avg_glucose_level'] < 260).select('id','gender','age','hypertension','heart_disease','work_type','Residence_type','ever_married','bmi','avg_glucose_level','smoking_status')\n",
    "c6.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Selecting columns to be considered during analysis\n",
    "clean = c6.select(['gender','age','hypertension','heart_disease','ever_married','work_type','Residence_type','avg_glucose_level','bmi','smoking_status'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39137"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(clean.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+------------+-------------+------------+-------------+--------------+-----------------+----+---------------+\n",
      "|gender| age|hypertension|heart_disease|ever_married|    work_type|Residence_type|avg_glucose_level| bmi| smoking_status|\n",
      "+------+----+------------+-------------+------------+-------------+--------------+-----------------+----+---------------+\n",
      "|  Male|58.0|           1|            0|         Yes|      Private|         Urban|            87.96|39.2|   never smoked|\n",
      "|Female|70.0|           0|            0|         Yes|      Private|         Rural|            69.04|35.9|formerly smoked|\n",
      "|Female|52.0|           0|            0|         Yes|      Private|         Urban|            77.59|17.7|formerly smoked|\n",
      "|Female|75.0|           0|            1|         Yes|Self-employed|         Rural|           243.53|27.0|   never smoked|\n",
      "|Female|32.0|           0|            0|         Yes|      Private|         Rural|            77.67|32.3|         smokes|\n",
      "|Female|37.0|           0|            0|         Yes|      Private|         Rural|           162.96|39.4|   never smoked|\n",
      "|Female|37.0|           0|            0|         Yes|      Private|         Rural|             73.5|26.1|formerly smoked|\n",
      "|Female|40.0|           0|            0|         Yes|      Private|         Rural|            95.04|42.4|   never smoked|\n",
      "|  Male|35.0|           0|            0|          No|      Private|         Rural|            85.37|33.0|   never smoked|\n",
      "|Female|20.0|           0|            0|          No|      Private|         Urban|            84.62|19.7|         smokes|\n",
      "|Female|42.0|           0|            0|         Yes|      Private|         Rural|            82.67|22.5|   never smoked|\n",
      "|Female|79.0|           0|            1|         Yes|Self-employed|         Urban|            67.84|25.2|         smokes|\n",
      "|Female|49.0|           0|            0|         Yes|      Private|         Rural|            60.22|31.5|         smokes|\n",
      "|  Male|71.0|           0|            0|         Yes|      Private|         Urban|           198.21|27.3|formerly smoked|\n",
      "|Female|59.0|           0|            0|         Yes|      Private|         Urban|           109.82|23.7|   never smoked|\n",
      "|Female|25.0|           0|            0|         Yes|      Private|         Urban|            60.84|24.5|   never smoked|\n",
      "|Female|67.0|           0|            0|         Yes|     Govt_job|         Rural|            94.61|28.4|         smokes|\n",
      "|Female|38.0|           0|            0|          No|      Private|         Rural|            97.49|26.9|   never smoked|\n",
      "|Female|54.0|           0|            0|         Yes|      Private|         Rural|           206.72|26.7|   never smoked|\n",
      "|Female|70.0|           0|            0|         Yes|Self-employed|         Rural|           214.45|31.2|   never smoked|\n",
      "+------+----+------------+-------------+------------+-------------+--------------+-----------------+----+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Displaying the clean data\n",
    "clean.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|summary|               age|\n",
      "+-------+------------------+\n",
      "|  count|             62001|\n",
      "|   mean|42.169475331042975|\n",
      "| stddev|22.522099535371346|\n",
      "|    min|              0.08|\n",
      "|    max|              82.0|\n",
      "+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Summarizing age attribute statistically\n",
    "concat.describe('age').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+\n",
      "|summary|avg_glucose_level|\n",
      "+-------+-----------------+\n",
      "|  count|            62001|\n",
      "|   mean| 104.453831712391|\n",
      "| stddev| 42.9605371949353|\n",
      "|    min|             55.0|\n",
      "|    max|           291.05|\n",
      "+-------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Summarizing avergae glucose level attribute statistically\n",
    "concat.describe('avg_glucose_level').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+\n",
      "|summary|              bmi|\n",
      "+-------+-----------------+\n",
      "|  count|            59948|\n",
      "|   mean| 28.5870988189766|\n",
      "| stddev|7.766122190731495|\n",
      "|    min|             10.1|\n",
      "|    max|             97.6|\n",
      "+-------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Summarizing bmi attribute statistically\n",
    "concat.describe('bmi').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39137"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Showing records after cleaning\n",
    "clean.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking accuracy of Linear Regression for data mining on the cleaned dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "ename": "IllegalArgumentException",
     "evalue": "'requirement failed: Output column varIdgender already exists.'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m~/spark-2.1.1-bin-hadoop2.7/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    318\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    320\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o2312.fit.\n: java.lang.IllegalArgumentException: requirement failed: Output column varIdgender already exists.\n\tat scala.Predef$.require(Predef.scala:224)\n\tat org.apache.spark.ml.feature.StringIndexerBase$class.validateAndTransformSchema(StringIndexer.scala:49)\n\tat org.apache.spark.ml.feature.StringIndexer.validateAndTransformSchema(StringIndexer.scala:66)\n\tat org.apache.spark.ml.feature.StringIndexer.transformSchema(StringIndexer.scala:99)\n\tat org.apache.spark.ml.PipelineStage.transformSchema(Pipeline.scala:74)\n\tat org.apache.spark.ml.feature.StringIndexer.fit(StringIndexer.scala:88)\n\tat org.apache.spark.ml.feature.StringIndexer.fit(StringIndexer.scala:66)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:748)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-227-87cb102e21e2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mStringIndexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOneHotEncoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mvarIdxer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStringIndexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputCol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'gender'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moutputCol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'varIdgender'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclean\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mclean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvarIdxer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclean\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark-2.1.1-bin-hadoop2.7/python/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m     62\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m             raise ValueError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
      "\u001b[0;32m~/spark-2.1.1-bin-hadoop2.7/python/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 236\u001b[0;31m         \u001b[0mjava_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    237\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjava_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark-2.1.1-bin-hadoop2.7/python/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    231\u001b[0m         \"\"\"\n\u001b[1;32m    232\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 233\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1133\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark-2.1.1-bin-hadoop2.7/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     77\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mQueryExecutionException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'java.lang.IllegalArgumentException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mIllegalArgumentException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m             \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIllegalArgumentException\u001b[0m: 'requirement failed: Output column varIdgender already exists.'"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder\n",
    "\n",
    "varIdxer = StringIndexer(inputCol='gender',outputCol='varIdgender').fit(clean)\n",
    "clean = varIdxer.transform(clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "varIdxer2 = StringIndexer(inputCol='work_type',outputCol='varIdworktype').fit(clean)\n",
    "clean = varIdxer2.transform(clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "varIdxer3 = StringIndexer(inputCol='Residence_type',outputCol='varIdresidencetype').fit(clean)\n",
    "clean = varIdxer3.transform(clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "varIdxer4 = StringIndexer(inputCol='smoking_status',outputCol='varIdsmokingstatus').fit(clean)\n",
    "clean = varIdxer4.transform(clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "varIdxer5 = StringIndexer(inputCol='ever_married',outputCol='varIdmarried').fit(clean)\n",
    "clean = varIdxer5.transform(clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "ename": "IllegalArgumentException",
     "evalue": "'requirement failed: Output column varCatgender already exists.'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m~/spark-2.1.1-bin-hadoop2.7/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    318\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    320\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o2390.transform.\n: java.lang.IllegalArgumentException: requirement failed: Output column varCatgender already exists.\n\tat scala.Predef$.require(Predef.scala:224)\n\tat org.apache.spark.ml.feature.OneHotEncoder.transformSchema(OneHotEncoder.scala:85)\n\tat org.apache.spark.ml.feature.OneHotEncoder.transform(OneHotEncoder.scala:141)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:748)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-232-7520ebbda5b5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mclean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOneHotEncoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputCol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"varIdgender\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputCol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"varCatgender\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclean\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/spark-2.1.1-bin-hadoop2.7/python/pyspark/ml/base.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    103\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Params must be a param map but got %s.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark-2.1.1-bin-hadoop2.7/python/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_transform\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    250\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 252\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1133\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark-2.1.1-bin-hadoop2.7/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     77\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mQueryExecutionException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'java.lang.IllegalArgumentException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mIllegalArgumentException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m             \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIllegalArgumentException\u001b[0m: 'requirement failed: Output column varCatgender already exists.'"
     ]
    }
   ],
   "source": [
    "clean = OneHotEncoder(inputCol=\"varIdgender\", outputCol=\"varCatgender\").transform(clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean = OneHotEncoder(inputCol=\"varIdworktype\", outputCol=\"varCatworktype\").transform(clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean = OneHotEncoder(inputCol=\"varIdresidencetype\", outputCol=\"varCatresidencetype\").transform(clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean = OneHotEncoder(inputCol=\"varIdsmokingstatus\", outputCol=\"varCatsmokingstatus\").transform(clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean = OneHotEncoder(inputCol=\"varIdmarried\", outputCol=\"varCatmarried\").transform(clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(\n",
    "    inputCols=['age','hypertension','bmi','avg_glucose_level','varCatgender','varCatworktype','varCatresidencetype','varCatsmokingstatus','varCatmarried'],\n",
    "    outputCol='features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = assembler.transform(clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- gender: string (nullable = true)\n",
      " |-- age: double (nullable = true)\n",
      " |-- hypertension: integer (nullable = true)\n",
      " |-- heart_disease: integer (nullable = true)\n",
      " |-- ever_married: string (nullable = true)\n",
      " |-- work_type: string (nullable = true)\n",
      " |-- Residence_type: string (nullable = true)\n",
      " |-- avg_glucose_level: double (nullable = true)\n",
      " |-- bmi: double (nullable = true)\n",
      " |-- smoking_status: string (nullable = true)\n",
      " |-- varIdgender: double (nullable = true)\n",
      " |-- varCatgender: vector (nullable = true)\n",
      " |-- varIdworktype: double (nullable = true)\n",
      " |-- varIdresidencetype: double (nullable = true)\n",
      " |-- varIdsmokingstatus: double (nullable = true)\n",
      " |-- varIdmarried: double (nullable = true)\n",
      " |-- varCatworktype: vector (nullable = true)\n",
      " |-- varCatresidencetype: vector (nullable = true)\n",
      " |-- varCatsmokingstatus: vector (nullable = true)\n",
      " |-- varCatmarried: vector (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(gender='Male', age=58.0, hypertension=1, heart_disease=0, ever_married='Yes', work_type='Private', Residence_type='Urban', avg_glucose_level=87.96, bmi=39.2, smoking_status='never smoked', varIdgender=1.0, varCatgender=SparseVector(2, {1: 1.0}), varIdworktype=0.0, varIdresidencetype=0.0, varIdsmokingstatus=0.0, varIdmarried=0.0, varCatworktype=SparseVector(4, {0: 1.0}), varCatresidencetype=SparseVector(1, {0: 1.0}), varCatsmokingstatus=SparseVector(2, {0: 1.0}), varCatmarried=SparseVector(1, {0: 1.0}), features=DenseVector([58.0, 1.0, 39.2, 87.96, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0]))]"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.printSchema()\n",
    "output.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------+\n",
      "|            features|heart_disease|\n",
      "+--------------------+-------------+\n",
      "|[58.0,1.0,39.2,87...|            0|\n",
      "|(14,[0,2,3,4,6,12...|            0|\n",
      "|(14,[0,2,3,4,6,10...|            0|\n",
      "|(14,[0,2,3,4,7,11...|            1|\n",
      "|(14,[0,2,3,4,6,13...|            0|\n",
      "|(14,[0,2,3,4,6,11...|            0|\n",
      "|(14,[0,2,3,4,6,12...|            0|\n",
      "|(14,[0,2,3,4,6,11...|            0|\n",
      "|(14,[0,2,3,5,6,11...|            0|\n",
      "|(14,[0,2,3,4,6,10...|            0|\n",
      "|(14,[0,2,3,4,6,11...|            0|\n",
      "|(14,[0,2,3,4,7,10...|            1|\n",
      "|(14,[0,2,3,4,6,13...|            0|\n",
      "|(14,[0,2,3,5,6,10...|            0|\n",
      "|(14,[0,2,3,4,6,10...|            0|\n",
      "|(14,[0,2,3,4,6,10...|            0|\n",
      "|(14,[0,2,3,4,8,13...|            0|\n",
      "|(14,[0,2,3,4,6,11...|            0|\n",
      "|(14,[0,2,3,4,6,11...|            0|\n",
      "|(14,[0,2,3,4,7,11...|            0|\n",
      "+--------------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final_data = output.select(\"features\",'heart_disease')\n",
    "final_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data,test_data = final_data.randomSplit([0.7,0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression(labelCol='heart_disease')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrModel = lr.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: [0.0028501976030749325,0.03461851737410233,-0.00025531360004540156,0.0003936265260828681,0.01589073368075396,0.050875704497625594,-0.03329056934113632,-0.030750133402180378,-0.04100244978601662,0.00625474317868066,-0.002594993802006564,-0.02487609516088766,-0.007226023065390142,-0.02713302653081059] Intercept: -0.08304128990893143\n"
     ]
    }
   ],
   "source": [
    "print(\"Coefficients: {} Intercept: {}\".format(lrModel.coefficients,lrModel.intercept))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results = lrModel.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|           residuals|\n",
      "+--------------------+\n",
      "| -0.0260998829549475|\n",
      "|-0.02668521378575...|\n",
      "|-0.09684262314219157|\n",
      "|-0.14713639081867635|\n",
      "|-0.18263822369203042|\n",
      "|-0.03317920652158...|\n",
      "|-0.07423482319493575|\n",
      "|-0.10209350704558375|\n",
      "| -0.2115206524550498|\n",
      "|-0.17325860987893144|\n",
      "|-0.18859979608581168|\n",
      "|-0.02796023585377297|\n",
      "|-0.01965744582246...|\n",
      "|-0.06454210030201545|\n",
      "|-0.06751108912554364|\n",
      "|-0.07586885380909629|\n",
      "|-0.11569166745194147|\n",
      "|-0.17527391923568403|\n",
      "| -0.2093389891296657|\n",
      "|-0.19760252423217844|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "RSME: 0.21542829447732822\n"
     ]
    }
   ],
   "source": [
    "test_results.residuals.show()\n",
    "print(\"RSME: {}\".format(test_results.rootMeanSquaredError))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2: 0.08661971831234527\n"
     ]
    }
   ],
   "source": [
    "print(\"R2: {}\".format(test_results.r2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking accuracy of Logistic Regression for data mining on the cleaned dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import (VectorAssembler,VectorIndexer,\n",
    "                                OneHotEncoder,StringIndexer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "gender_indexer = StringIndexer(inputCol='gender',outputCol='GenderIndex')\n",
    "gender_encoder = OneHotEncoder(inputCol='GenderIndex',outputCol='GenderVec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "work_indexer = StringIndexer(inputCol='work_type',outputCol='WorkTypeIndex')\n",
    "work_encoder = OneHotEncoder(inputCol='WorkTypeIndex',outputCol='WorkTypeVec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "married_indexer = StringIndexer(inputCol='ever_married',outputCol='EverMarriedIndex')\n",
    "married_encoder = OneHotEncoder(inputCol='EverMarriedIndex',outputCol='EverMarriedVec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "residence_indexer = StringIndexer(inputCol='Residence_type',outputCol='ResidenceTypeIndex')\n",
    "residence_encoder = OneHotEncoder(inputCol='ResidenceTypeIndex',outputCol='ResidenceTypeVec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "smoking_indexer = StringIndexer(inputCol='smoking_status',outputCol='SmokingStatusIndex')\n",
    "smoking_encoder = OneHotEncoder(inputCol='SmokingStatusIndex',outputCol='SmokingStatusVec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(inputCols=['age',\n",
    " 'GenderVec',\n",
    " 'bmi',\n",
    " 'avg_glucose_level',\n",
    " 'hypertension',\n",
    " 'WorkTypeVec',\n",
    " 'EverMarriedVec','ResidenceTypeVec', 'SmokingStatusVec'],outputCol='features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg_heart = LogisticRegression(featuresCol='features',labelCol='heart_disease')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(stages=[gender_indexer,work_indexer,married_indexer,residence_indexer,smoking_indexer,\n",
    "                           gender_encoder,work_encoder,married_encoder,residence_encoder,smoking_encoder,\n",
    "                           assembler,log_reg_heart])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_heart_data, test_heart_data = clean.randomSplit([0.7,.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_model = pipeline.fit(train_heart_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = fit_model.transform(test_heart_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "my_eval = BinaryClassificationEvaluator(rawPredictionCol='prediction',\n",
    "                                       labelCol='heart_disease')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+\n",
      "|heart_disease|prediction|\n",
      "+-------------+----------+\n",
      "|            0|       0.0|\n",
      "|            0|       0.0|\n",
      "|            0|       0.0|\n",
      "|            0|       0.0|\n",
      "|            0|       0.0|\n",
      "|            0|       0.0|\n",
      "|            0|       0.0|\n",
      "|            0|       0.0|\n",
      "|            0|       0.0|\n",
      "|            0|       0.0|\n",
      "|            0|       0.0|\n",
      "|            0|       0.0|\n",
      "|            0|       0.0|\n",
      "|            0|       0.0|\n",
      "|            0|       0.0|\n",
      "|            0|       0.0|\n",
      "|            0|       0.0|\n",
      "|            0|       0.0|\n",
      "|            0|       0.0|\n",
      "|            0|       0.0|\n",
      "+-------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results.select('heart_disease','prediction').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5046344461960476"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AUC = my_eval.evaluate(results)\n",
    "\n",
    "AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct: 11592\n",
      "Total Correct: 10976\n"
     ]
    }
   ],
   "source": [
    "totalResults = results.select('heart_disease','prediction')\n",
    "\n",
    "correctResults = totalResults.filter(totalResults['heart_disease'] == totalResults['prediction'])\n",
    "\n",
    "countTR = totalResults.count()\n",
    "print(\"Correct: \" + str(countTR))\n",
    "\n",
    "countTC = correctResults.count()\n",
    "print(\"Total Correct: \" + str(countTC)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking accuracy of Tree model(Single decision tree, random forest, Gradient boosted tree) for data mining on the cleaned dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "#varIdxer = StringIndexer(inputCol='gender',outputCol='vargender').fit(clean)\n",
    "#clean = varIdxer.transform(clean)\n",
    "#varIdxer2 = StringIndexer(inputCol='work_type',outputCol='varworktype').fit(clean)\n",
    "#clean = varIdxer2.transform(clean)\n",
    "#varIdxer3 = StringIndexer(inputCol='Residence_type',outputCol='varresidencetype').fit(clean)\n",
    "#clean = varIdxer3.transform(clean)\n",
    "#varIdxer4 = StringIndexer(inputCol='smoking_status',outputCol='varsmokingstatus').fit(clean)\n",
    "#clean = varIdxer4.transform(clean)\n",
    "#varIdxer5 = StringIndexer(inputCol='ever_married',outputCol='varmarried').fit(clean)\n",
    "#clean = varIdxer5.transform(clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean = OneHotEncoder(inputCol=\"vargender\", outputCol=\"varcatgender\").transform(clean)\n",
    "#clean = OneHotEncoder(inputCol=\"varworktype\", outputCol=\"varcatworktype\").transform(clean)\n",
    "#clean = OneHotEncoder(inputCol=\"varresidencetype\", outputCol=\"varcatresidencetype\").transform(clean)\n",
    "#clean = OneHotEncoder(inputCol=\"varsmokingstatus\", outputCol=\"varcatsmokingstatus\").transform(clean)\n",
    "#clean = OneHotEncoder(inputCol=\"varmarried\", outputCol=\"varcatmarried\").transform(clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(\n",
    "  inputCols=['age',\n",
    "             'bmi',\n",
    "             'avg_glucose_level',\n",
    "             'hypertension','varCatgender','varCatworktype','varCatresidencetype','varCatsmokingstatus','varCatmarried'],\n",
    "              outputCol=\"features1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "\"Reference 'varCatgender' is ambiguous, could be: varCatgender#5809, varCatgender#7050.;\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m~/spark-2.1.1-bin-hadoop2.7/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    318\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    320\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o2961.transform.\n: org.apache.spark.sql.AnalysisException: Reference 'varCatgender' is ambiguous, could be: varCatgender#5809, varCatgender#7050.;\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolve(LogicalPlan.scala:264)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveQuoted(LogicalPlan.scala:168)\n\tat org.apache.spark.sql.Dataset.resolve(Dataset.scala:217)\n\tat org.apache.spark.sql.Dataset.col(Dataset.scala:1083)\n\tat org.apache.spark.sql.Dataset.apply(Dataset.scala:1069)\n\tat org.apache.spark.ml.feature.VectorAssembler$$anonfun$4.apply(VectorAssembler.scala:104)\n\tat org.apache.spark.ml.feature.VectorAssembler$$anonfun$4.apply(VectorAssembler.scala:101)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)\n\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n\tat scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186)\n\tat org.apache.spark.ml.feature.VectorAssembler.transform(VectorAssembler.scala:101)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:748)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-275-24e4db27d3cc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0massembler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclean\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/spark-2.1.1-bin-hadoop2.7/python/pyspark/ml/base.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    103\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Params must be a param map but got %s.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark-2.1.1-bin-hadoop2.7/python/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_transform\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    250\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 252\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1133\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark-2.1.1-bin-hadoop2.7/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: \"Reference 'varCatgender' is ambiguous, could be: varCatgender#5809, varCatgender#7050.;\""
     ]
    }
   ],
   "source": [
    "output = assembler.transform(clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexer = StringIndexer(inputCol=\"heart_disease\", outputCol=\"HeartDiseaseIndex\")\n",
    "output_fixed = indexer.fit(output).transform(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data = output_fixed.select(\"features1\",'HeartDiseaseIndex')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "train1_data,test1_data = final_data.randomSplit([0.7,0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import DecisionTreeClassifier,GBTClassifier,RandomForestClassifier\n",
    "from pyspark.ml import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtc = DecisionTreeClassifier(labelCol='HeartDiseaseIndex',featuresCol='features1')\n",
    "rfc = RandomForestClassifier(labelCol='HeartDiseaseIndex',featuresCol='features1')\n",
    "gbt = GBTClassifier(labelCol='HeartDiseaseIndex',featuresCol='features1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtc_model = dtc.fit(train1_data)\n",
    "rfc_model = rfc.fit(train1_data)\n",
    "gbt_model = gbt.fit(train1_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtc_predictions = dtc_model.transform(test1_data)\n",
    "rfc_predictions = rfc_model.transform(test1_data)\n",
    "gbt_predictions = gbt_model.transform(test1_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "my_binary_eval = BinaryClassificationEvaluator(labelCol = 'HeartDiseaseIndex')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DTC\n",
      "0.7561761225548912\n",
      "RFC\n",
      "0.8435945561792932\n",
      "GBT\n",
      "0.5007533993471214\n"
     ]
    }
   ],
   "source": [
    "print(\"DTC\")\n",
    "print(my_binary_eval.evaluate(dtc_predictions))\n",
    "\n",
    "print(\"RFC\")\n",
    "print(my_binary_eval.evaluate(rfc_predictions))\n",
    "\n",
    "my_binary_gbt_eval = BinaryClassificationEvaluator(labelCol='HeartDiseaseIndex', rawPredictionCol='prediction')\n",
    "print(\"GBT\")\n",
    "print(my_binary_gbt_eval.evaluate(gbt_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_evaluator = MulticlassClassificationEvaluator(labelCol=\"HeartDiseaseIndex\", predictionCol=\"prediction\", metricName=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtc_acc = acc_evaluator.evaluate(dtc_predictions)\n",
    "rfc_acc = acc_evaluator.evaluate(rfc_predictions)\n",
    "gbt_acc = acc_evaluator.evaluate(gbt_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are the results!\n",
      "----------------------------------------\n",
      "A single decision tree has an accuracy of: 94.63%\n",
      "----------------------------------------\n",
      "A random forest ensemble has an accuracy of: 94.63%\n",
      "----------------------------------------\n",
      "An ensemble using GBT has an accuracy of: 94.63%\n"
     ]
    }
   ],
   "source": [
    "print(\"Here are the results!\")\n",
    "print('-'*40)\n",
    "print('A single decision tree has an accuracy of: {0:2.2f}%'.format(dtc_acc*100))\n",
    "print('-'*40)\n",
    "print('A random forest ensemble has an accuracy of: {0:2.2f}%'.format(rfc_acc*100))\n",
    "print('-'*40)\n",
    "print('An ensemble using GBT has an accuracy of: {0:2.2f}%'.format(gbt_acc*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------------+--------------------+--------------------+----------+\n",
      "|           features1|HeartDiseaseIndex|       rawPrediction|         probability|prediction|\n",
      "+--------------------+-----------------+--------------------+--------------------+----------+\n",
      "|(14,[0,1,2,3,4,6]...|              0.0|[19.5385070109148...|[0.97692535054574...|       0.0|\n",
      "|(14,[0,1,2,3,4,6]...|              0.0|[18.2777262992536...|[0.91388631496267...|       0.0|\n",
      "|(14,[0,1,2,3,4,6,...|              0.0|[19.5492914096524...|[0.97746457048262...|       0.0|\n",
      "|(14,[0,1,2,3,4,6,...|              0.0|[19.4116417380960...|[0.97058208690480...|       0.0|\n",
      "|(14,[0,1,2,3,4,6,...|              0.0|[19.1348908454609...|[0.95674454227304...|       0.0|\n",
      "|(14,[0,1,2,3,4,6,...|              0.0|[19.7571767526532...|[0.98785883763266...|       0.0|\n",
      "|(14,[0,1,2,3,4,6,...|              0.0|[19.7456165971685...|[0.98728082985842...|       0.0|\n",
      "|(14,[0,1,2,3,4,6,...|              0.0|[19.2095301170337...|[0.96047650585168...|       0.0|\n",
      "|(14,[0,1,2,3,4,6,...|              0.0|[19.6317369758482...|[0.98158684879240...|       0.0|\n",
      "|(14,[0,1,2,3,4,6,...|              0.0|[18.4737309629680...|[0.92368654814840...|       0.0|\n",
      "|(14,[0,1,2,3,4,6,...|              0.0|[16.9578118963714...|[0.84789059481857...|       0.0|\n",
      "|(14,[0,1,2,3,4,6,...|              0.0|[17.8009556394480...|[0.89004778197240...|       0.0|\n",
      "|(14,[0,1,2,3,4,6,...|              0.0|[18.7225867313271...|[0.93612933656635...|       0.0|\n",
      "|(14,[0,1,2,3,4,6,...|              0.0|[17.7848026727163...|[0.88924013363581...|       0.0|\n",
      "|(14,[0,1,2,3,4,6,...|              0.0|[16.7961140989299...|[0.83980570494649...|       0.0|\n",
      "|(14,[0,1,2,3,4,6,...|              0.0|[19.3895952683435...|[0.96947976341717...|       0.0|\n",
      "|(14,[0,1,2,3,4,6,...|              0.0|[18.2304823578366...|[0.91152411789183...|       0.0|\n",
      "|(14,[0,1,2,3,4,6,...|              0.0|[19.3537818461391...|[0.96768909230695...|       0.0|\n",
      "|(14,[0,1,2,3,4,6,...|              1.0|[19.3072132260761...|[0.96536066130380...|       0.0|\n",
      "|(14,[0,1,2,3,4,6,...|              0.0|[19.3072132260761...|[0.96536066130380...|       0.0|\n",
      "+--------------------+-----------------+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rfc_predictions.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
